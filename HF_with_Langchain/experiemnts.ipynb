{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain_huggingface) (0.26.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain_huggingface) (0.3.28)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain_huggingface) (3.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain_huggingface) (0.20.3)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain_huggingface) (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jayraj\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
      "Requirement already satisfied: networkx in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayraj\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\jayraj\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jayraj\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (0.26.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayraj\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jayraj\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "## when we have to so API Call then also huggingface_hub is require \n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint  # if you want to acces the model then you require hf endpoint \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "c:\\Users\\Jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? Machine learning is a method of data analysis that automates analytical model building. It’s a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.\\n\\nMachine learning is used in a wide range of applications, including email filtering, detection of network intruders, and computer vision. It’s also used in marketing automation to help businesses better understand their customers and improve their marketing efforts.\\n\\nThere are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\\n\\n1. Supervised learning: In supervised learning, the machine learning algorithm is trained on a labeled dataset. This means that the data is already classified or labeled, and the algorithm learns to make predictions based on this labeled data. For example, if you want to build a model to predict whether an email is spam or not, you would feed the algorithm a dataset of emails that have already been labeled as spam or not spam.\\n2. Unsupervised learning: In unsupervised learning, the algorithm is not given any labeled data. Instead, it must find patterns and structure in the data on its own. This is often used for tasks like clustering, where the goal is to group similar data points together. For example, you might use unsupervised learning to group customers into segments based on their purchasing behavior.\\n3. Reinforcement learning: In reinforcement learning, the algorithm learns by interacting with an environment and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes the total reward over time. This is often used in robotics and game playing, where the agent must learn to take actions that lead to the best outcome.\\n\\nMachine learning is a powerful tool that can help businesses make better decisions, improve their operations, and better understand their customers. However, it’s important to remember that machine learning is not a replacement for human judgment, but rather a tool that can help inform and support decision-making. It’s also important to carefully consider the ethical implications of using machine learning, such as issues related to privacy and fairness.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'❓\\n\\nGenerative AI refers to a type of artificial intelligence that can create new content, such as images, music, or text, based on patterns learned from existing data. Generative AI models use techniques such as deep learning, neural networks, and reinforcement learning to generate new content that is similar to, but not identical to, the data they were trained on. Some examples of generative AI include deepfakes, AI-generated art, and AI-written stories.\\n\\nGenerative AI has a wide range of potential applications, including creating personalized content for users, generating new ideas for designers and creators, and automating tasks such as data entry and content creation. However, generative AI also raises ethical concerns, such as the potential for misuse in creating deepfakes or generating offensive or harmful content. It is important for developers and users of generative AI to be aware of these potential risks and to take steps to mitigate them.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\win10\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": " (Request ID: fxSl_33ZiXHMdaimyWjqv)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content,make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1322\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1321\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1322\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:258\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32me:\\UDemy Final\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m:  (Request ID: fxSl_33ZiXHMdaimyWjqv)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content,make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\win10\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jayraj\\AppData\\Local\\Temp\\ipykernel_28372\\3735178259.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain=LLMChain(llm=llm,prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"? India won the cricket World Cup 2011, defeating Sri Lanka in the final match held on April 2, 2011, at the Wankhede Stadium in Mumbai, India. India scored 277 runs for the loss of 6 wickets in 50 overs, and Sri Lanka was all out for 275 runs in 49.5 overs. This was India's second World Cup victory, having previously won the tournament in 1983.\\n\\nThe match was a closely contested one, with both teams giving a tough fight. India's captain, Mahendra Singh Dhoni, was named the Man of the Match for his unbeaten 91 runs off 79 balls, which played a crucial role in India's victory. Yuvraj Singh was named the Man of the Series for his outstanding performance throughout the tournament, including several match-winning innings and crucial wickets with the ball.\\n\\nThe World Cup final was a historic event for Indian cricket, as it was the first time that a World Cup final was played in India. The match was attended by a record-breaking crowd of over 33,000 spectators, and was watched by millions of people across the world. The victory was a source of immense joy and pride for Indian cricket fans, and marked a significant milestone in the history of Indian sports.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jayraj\\AppData\\Local\\Temp\\ipykernel_28372\\2593296450.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceBgeEmbeddings(\n",
      "c:\\Users\\Jayraj\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jayraj\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02841656096279621,\n",
       " 0.012183279730379581,\n",
       " 0.027443930506706238,\n",
       " -0.05482868477702141,\n",
       " 0.024238910526037216,\n",
       " 0.0007663055439479649,\n",
       " 0.06783363968133926,\n",
       " 0.01634834334254265,\n",
       " -0.018950743600726128,\n",
       " 0.01254290621727705,\n",
       " 0.02156500145792961,\n",
       " -0.08793038874864578,\n",
       " 0.0006460713339038193,\n",
       " 0.03327082097530365,\n",
       " 0.005463753826916218,\n",
       " -0.060376472771167755,\n",
       " 0.050422657281160355,\n",
       " 0.004434817470610142,\n",
       " 0.0009598946198821068,\n",
       " 0.0017405522521585226,\n",
       " 0.003298833966255188,\n",
       " 0.03167250379920006,\n",
       " -0.04880747199058533,\n",
       " -0.04481915012001991,\n",
       " 0.07132110744714737,\n",
       " -0.007510860450565815,\n",
       " -0.0011259395396336913,\n",
       " -0.01580120250582695,\n",
       " -0.029402371495962143,\n",
       " -0.17224568128585815,\n",
       " -0.031895194202661514,\n",
       " -0.001629163627512753,\n",
       " 0.01810498908162117,\n",
       " 0.015315390191972256,\n",
       " -0.02072959579527378,\n",
       " -0.008872960694134235,\n",
       " -0.0012822661083191633,\n",
       " 0.027276931330561638,\n",
       " -0.010114241391420364,\n",
       " 0.012621615082025528,\n",
       " -0.007077896036207676,\n",
       " -0.01669314317405224,\n",
       " 0.04085584729909897,\n",
       " 0.023938355967402458,\n",
       " -0.02008151449263096,\n",
       " 0.028681157156825066,\n",
       " -0.019400743767619133,\n",
       " -0.01461820863187313,\n",
       " 0.017379628494381905,\n",
       " 0.0041640824638307095,\n",
       " 0.06415650248527527,\n",
       " 0.04768307879567146,\n",
       " 0.0018365145660936832,\n",
       " -8.070914191193879e-05,\n",
       " 0.01659681461751461,\n",
       " 0.011124175041913986,\n",
       " 0.069694384932518,\n",
       " 0.05182047560811043,\n",
       " 0.05568530783057213,\n",
       " 0.05551543086767197,\n",
       " 0.0005039662355557084,\n",
       " 0.0418706014752388,\n",
       " -0.15344086289405823,\n",
       " 0.05180780589580536,\n",
       " 0.006689799949526787,\n",
       " -0.031670715659856796,\n",
       " -0.009105012752115726,\n",
       " -0.051604725420475006,\n",
       " 0.042508602142333984,\n",
       " 0.0282000545412302,\n",
       " -0.010748140513896942,\n",
       " 0.022405795753002167,\n",
       " 0.04439552500844002,\n",
       " 0.0041155098006129265,\n",
       " 0.018998488783836365,\n",
       " -0.0043571749702095985,\n",
       " 0.04762764647603035,\n",
       " 0.011824607849121094,\n",
       " 0.008164585568010807,\n",
       " 0.008177277632057667,\n",
       " -0.009698750451207161,\n",
       " -0.014260290190577507,\n",
       " 0.01140968780964613,\n",
       " -0.07362116128206253,\n",
       " -0.05439521744847298,\n",
       " -0.05703964829444885,\n",
       " -0.0036085473839193583,\n",
       " 0.0026660647708922625,\n",
       " 0.023782482370734215,\n",
       " 0.015376215800642967,\n",
       " -0.07020369917154312,\n",
       " -0.03130036219954491,\n",
       " -0.0031142907682806253,\n",
       " -0.015812186524271965,\n",
       " -0.03791401535272598,\n",
       " -0.025921916589140892,\n",
       " 0.018168391659855843,\n",
       " -0.03882461413741112,\n",
       " -0.05674504116177559,\n",
       " 0.5792059898376465,\n",
       " -0.05278833210468292,\n",
       " 0.020716369152069092,\n",
       " 0.06794390082359314,\n",
       " -0.04541652277112007,\n",
       " 0.011642470955848694,\n",
       " -0.021571751683950424,\n",
       " 0.020341726019978523,\n",
       " -0.027448944747447968,\n",
       " -0.045588940382003784,\n",
       " -0.02944357320666313,\n",
       " -0.023662518709897995,\n",
       " -0.034315239638090134,\n",
       " 0.0019388552755117416,\n",
       " -0.07095140963792801,\n",
       " 0.03455634042620659,\n",
       " -0.030558962374925613,\n",
       " 0.039078596979379654,\n",
       " -0.02970731258392334,\n",
       " -0.0008282964117825031,\n",
       " -0.01215934194624424,\n",
       " -0.018272830173373222,\n",
       " 0.02548649162054062,\n",
       " -0.004461659584194422,\n",
       " 0.016335316002368927,\n",
       " 0.019126495346426964,\n",
       " -0.05483204126358032,\n",
       " 0.02763599343597889,\n",
       " -0.004757691640406847,\n",
       " 0.05900176241993904,\n",
       " -0.0016944841481745243,\n",
       " 0.008015005849301815,\n",
       " -0.037726838141679764,\n",
       " -0.09893041849136353,\n",
       " -0.022574391216039658,\n",
       " -0.037604644894599915,\n",
       " -0.0021698756609112024,\n",
       " 0.003244602121412754,\n",
       " -0.019202522933483124,\n",
       " -0.008631240576505661,\n",
       " -0.048023100942373276,\n",
       " 0.008696703240275383,\n",
       " -0.09516111761331558,\n",
       " -0.03496047109365463,\n",
       " -0.04360802844166756,\n",
       " -0.00034401644370518625,\n",
       " -0.010173632763326168,\n",
       " -0.030999524518847466,\n",
       " 0.024309681728482246,\n",
       " -0.020402051508426666,\n",
       " 0.031139379367232323,\n",
       " 0.0008811174775473773,\n",
       " 0.013916493393480778,\n",
       " -0.031196201220154762,\n",
       " -0.037154048681259155,\n",
       " 0.004029625561088324,\n",
       " 0.014799795113503933,\n",
       " 0.043188951909542084,\n",
       " 0.038754820823669434,\n",
       " 0.013851981610059738,\n",
       " 0.01979786343872547,\n",
       " 0.010267077945172787,\n",
       " -0.005434116814285517,\n",
       " -0.014299213886260986,\n",
       " 0.027637850493192673,\n",
       " 0.009802636690437794,\n",
       " -0.13550284504890442,\n",
       " -0.01713976450264454,\n",
       " 0.017617102712392807,\n",
       " 0.023132236674427986,\n",
       " 0.001759010716341436,\n",
       " 0.03088941052556038,\n",
       " 0.03991871327161789,\n",
       " -0.013684185221791267,\n",
       " 0.02481653355062008,\n",
       " 0.05405015870928764,\n",
       " 0.017761169001460075,\n",
       " -0.018475061282515526,\n",
       " 0.02595539577305317,\n",
       " -0.006377533543854952,\n",
       " -0.016587311401963234,\n",
       " 0.03784802183508873,\n",
       " -0.027290044352412224,\n",
       " -0.0528457947075367,\n",
       " -0.038033172488212585,\n",
       " 0.05191108584403992,\n",
       " -0.007557060103863478,\n",
       " -0.031805310398340225,\n",
       " 0.013284190557897091,\n",
       " -0.027723709121346474,\n",
       " 0.05630655214190483,\n",
       " 0.0030418778769671917,\n",
       " 0.05332484468817711,\n",
       " -0.05791127309203148,\n",
       " -0.011325813829898834,\n",
       " -0.031172040849924088,\n",
       " 0.025608684867620468,\n",
       " 0.03389057517051697,\n",
       " -0.0010284347226843238,\n",
       " 0.015864888206124306,\n",
       " 0.010595189407467842,\n",
       " -0.02703780308365822,\n",
       " -0.0009308445733040571,\n",
       " -0.04815223813056946,\n",
       " 0.02817925252020359,\n",
       " 0.010320624336600304,\n",
       " 0.06662959605455399,\n",
       " -0.016558196395635605,\n",
       " -0.004431402776390314,\n",
       " 0.03823423758149147,\n",
       " -0.02340821363031864,\n",
       " -0.035581737756729126,\n",
       " -0.05829068273305893,\n",
       " -0.011181505396962166,\n",
       " -0.01768455281853676,\n",
       " -0.016141286119818687,\n",
       " -0.03424530476331711,\n",
       " -0.02513953484594822,\n",
       " 0.03939666226506233,\n",
       " -0.02365824021399021,\n",
       " -0.007725009229034185,\n",
       " -0.005098926369100809,\n",
       " -0.03523438051342964,\n",
       " -0.014076839201152325,\n",
       " -0.2232602834701538,\n",
       " -0.03147139027714729,\n",
       " -0.0012905950425192714,\n",
       " -0.0017199987778440118,\n",
       " -0.007846050895750523,\n",
       " -0.05802324786782265,\n",
       " 0.0461745522916317,\n",
       " 0.024552645161747932,\n",
       " 0.07320842146873474,\n",
       " 0.017268339172005653,\n",
       " 0.04761209711432457,\n",
       " 0.013473288156092167,\n",
       " -0.005516032688319683,\n",
       " -0.014357813633978367,\n",
       " -0.009674289263784885,\n",
       " 0.048782505095005035,\n",
       " 0.030538097023963928,\n",
       " -0.024993950501084328,\n",
       " 0.02148621529340744,\n",
       " 0.017639825120568275,\n",
       " 0.05313889682292938,\n",
       " 0.013485007919371128,\n",
       " -0.023225978016853333,\n",
       " -0.021403955295681953,\n",
       " 0.0260753370821476,\n",
       " 0.002029207767918706,\n",
       " 0.12753744423389435,\n",
       " 0.08316833525896072,\n",
       " 0.04408947005867958,\n",
       " -0.02670358307659626,\n",
       " 0.00552201084792614,\n",
       " -0.00929486844688654,\n",
       " 0.02007429115474224,\n",
       " -0.09684175252914429,\n",
       " -0.02470392733812332,\n",
       " 0.02508697286248207,\n",
       " 0.0020886200945824385,\n",
       " -0.0448940210044384,\n",
       " -0.07861138880252838,\n",
       " -0.004376362077891827,\n",
       " -0.06590455025434494,\n",
       " 0.014689406380057335,\n",
       " -0.05764186754822731,\n",
       " -0.07152026146650314,\n",
       " -0.06232647970318794,\n",
       " 0.003431678283959627,\n",
       " -0.04606551304459572,\n",
       " 0.045300889760255814,\n",
       " -0.026762260124087334,\n",
       " 0.034010909497737885,\n",
       " 0.04547383636236191,\n",
       " -0.028179239481687546,\n",
       " 0.005011791363358498,\n",
       " 0.00963082816451788,\n",
       " -0.030305558815598488,\n",
       " -0.03612479194998741,\n",
       " -0.013626978732645512,\n",
       " -0.032653722912073135,\n",
       " -0.04467751085758209,\n",
       " 0.010642154142260551,\n",
       " -0.027486367151141167,\n",
       " -0.02456512115895748,\n",
       " -0.02474774606525898,\n",
       " 0.053619567304849625,\n",
       " 0.020789967849850655,\n",
       " 0.019468482583761215,\n",
       " 0.053241197019815445,\n",
       " -0.014002466574311256,\n",
       " 0.02124325931072235,\n",
       " -0.04957321658730507,\n",
       " -0.008522605523467064,\n",
       " 0.007852854207158089,\n",
       " -0.05719393864274025,\n",
       " -0.027550633996725082,\n",
       " 0.005300914868712425,\n",
       " 0.04007292538881302,\n",
       " 0.01959792897105217,\n",
       " -0.04519733041524887,\n",
       " 0.032435812056064606,\n",
       " -0.012342429719865322,\n",
       " 0.034314412623643875,\n",
       " 0.02110210806131363,\n",
       " 0.03984646499156952,\n",
       " 0.03166380524635315,\n",
       " -0.03359023854136467,\n",
       " 0.03164784610271454,\n",
       " -0.003304522717371583,\n",
       " 0.004641841631382704,\n",
       " 0.03758934140205383,\n",
       " -0.05924459174275398,\n",
       " 0.007028359919786453,\n",
       " 0.0038087167777121067,\n",
       " -0.025788895785808563,\n",
       " -0.02120332606136799,\n",
       " 0.022691233083605766,\n",
       " -0.021772973239421844,\n",
       " -0.27963775396347046,\n",
       " 0.007267446257174015,\n",
       " 0.021072011440992355,\n",
       " 0.04519745334982872,\n",
       " -0.020534491166472435,\n",
       " 0.02431371808052063,\n",
       " -0.0006136518204584718,\n",
       " -0.011857028119266033,\n",
       " -0.03296775743365288,\n",
       " 0.035843126475811005,\n",
       " 0.031281728297472,\n",
       " 0.06373954564332962,\n",
       " 0.046547841280698776,\n",
       " -0.014470555819571018,\n",
       " 0.01586972549557686,\n",
       " 0.03397124260663986,\n",
       " 0.018059568479657173,\n",
       " 0.002298760926350951,\n",
       " 0.016549888998270035,\n",
       " -0.021714895963668823,\n",
       " -0.03486000373959541,\n",
       " -0.0008649466908536851,\n",
       " 0.15126043558120728,\n",
       " -0.02453678473830223,\n",
       " 0.030671171844005585,\n",
       " -0.007318221032619476,\n",
       " -0.006135482341051102,\n",
       " 0.06415140628814697,\n",
       " 0.0160214900970459,\n",
       " -0.03636442869901657,\n",
       " 0.019898606464266777,\n",
       " -0.02117234095931053,\n",
       " 0.04829413443803787,\n",
       " -0.04478095471858978,\n",
       " 0.04763379693031311,\n",
       " 0.0007749418145976961,\n",
       " -0.005927972495555878,\n",
       " 0.061542682349681854,\n",
       " 0.023968402296304703,\n",
       " 0.013305016793310642,\n",
       " 0.022684471681714058,\n",
       " 0.014538079500198364,\n",
       " -0.05215905234217644,\n",
       " -0.032749637961387634,\n",
       " 0.08583346009254456,\n",
       " -0.0037248271983116865,\n",
       " 0.0013494156301021576,\n",
       " 0.04091988503932953,\n",
       " 0.011659668758511543,\n",
       " 0.05843622237443924,\n",
       " -0.0222861859947443,\n",
       " -0.011520694009959698,\n",
       " 0.004705701023340225,\n",
       " 0.0471825934946537,\n",
       " -0.0019179185619577765,\n",
       " 0.033009354025125504,\n",
       " -0.03505057096481323,\n",
       " -0.020736588165163994,\n",
       " -0.009222185239195824,\n",
       " 0.014618263579905033,\n",
       " 0.006456061732023954,\n",
       " 0.0010978468926623464,\n",
       " 0.01022400427609682,\n",
       " 0.08537217229604721,\n",
       " 0.03883955627679825]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
